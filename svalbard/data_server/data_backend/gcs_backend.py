"""Data Storage Backend that uses Google Cloud Storage"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path, PurePath, PurePosixPath
from typing import Literal

import zarr
from gcsfs import GCSFileSystem
from gcsfs.core import DEFAULT_PROJECT as GCSFS_DEFAULT_PROJECT

from .fs_backend import FSBackend, FSBackendConfig, Scopes


class GCSBackend(FSBackend):
    """Data Storage Backend that uses Google Cloud Storage"""

    def __init__(
        self,
        filesystem: GCSFileSystem,
        executor: ThreadPoolExecutor | None = None,
        loop: asyncio.AbstractEventLoop | None = None,
        path_base: Path = Path("/"),
    ) -> None:
        super().__init__(filesystem, executor, loop, path_base)
        self.filesystem.ls(
            str(self.path_base)
        )  # raises filenotfound error if bucket at pathbase is not found

    @property
    def filesystem(self) -> GCSFileSystem:
        assert isinstance(self._fs, GCSFileSystem)
        return self._fs

    async def _create_group(self, path: PurePath) -> zarr.Group:
        self.logger.debug(
            "GCSBackend: Creating group at path : %s", PurePosixPath(path.as_posix())
        )
        return await super()._create_group(PurePosixPath(path.as_posix()))

    async def _open_group(self, path: PurePath) -> zarr.Group:
        """open a group that already exists

        Args:
            path (Path): location of group in filesystem

        Returns:
            zarr.Group: a zarr group located at path
        """
        return await super()._open_group(PurePosixPath(path.as_posix()))

    async def _exists(self, path: PurePath) -> bool:
        return await self.loop.run_in_executor(
            self.executor, self.filesystem.exists, str(path.as_posix())
        )

    async def _ls(self, path: PurePath) -> list[Path]:
        # pylint: disable=protected-access
        files = await self.filesystem._ls(str(path.as_posix()), detail=False)
        return [PurePosixPath(file) for file in files]  # type: ignore


class GCSBackendConfig(FSBackendConfig):
    """Configuration for GCSFileSystem Based Data Backend

    Useful info from the GCSFileSystem docstring

    project : string
        project_id to work under. Note that this is not the same as, but often
        very similar to, the project name.
        This is required in order
        to list all the buckets you have access to within a project and to
        create/delete buckets, or update their access policies.
        If ``token='google_default'``, the value is overridden by the default,
        if ``token='anon'``, the value is ignored.
    access : one of {'read_only', 'read_write', 'full_control'}
        Full control implies read/write as well as modifying metadata,
        e.g., access control.
    token: None, dict or string
        - ``token=None``, GCSFS will attempt to guess your credentials in the
        following order: gcloud CLI default, gcsfs cached token, google compute
        metadata service, anonymous.
        - ``token='google_default'``, your default gcloud credentials will be used,
        which are typically established by doing ``gcloud login`` in a terminal.
        - ``token=='cache'``, credentials from previously successful gcsfs
        authentication will be used (use this after "browser" auth succeeded)
        - ``token='anon'``, no authentication is performed, and you can only
        access data which is accessible to allUsers (in this case, the project and
        access level parameters are meaningless)
        - ``token='browser'``, you get an access code with which you can
        authenticate via a specially provided URL
        - if ``token='cloud'``, we assume we are running within google compute
        or google container engine, and query the internal metadata directly for
        a token.
        - you may supply a token generated by the
        [gcloud](https://cloud.google.com/sdk/docs/)
        utility; this is either a python dictionary, the name of a file
        containing the JSON returned by logging in with the gcloud CLI tool,
        or a Credentials object. gcloud typically stores its tokens in locations
        such as
        ``~/.config/gcloud/application_default_credentials.json``,
        `` ~/.config/gcloud/credentials``, or
        ``~\\AppData\\Roaming\\gcloud\\credentials``, etc.
    endpoint_url: str
        If given, use this URL (format protocol://host:port , *without* any
        path part) for communication. If not given, defaults to the value
        of environment variable "STORAGE_EMULATOR_HOST"; if that is not set
        either, will use the standard Google endpoint.
    """

    cls: str = "gcsfs.core.GCSFileSystem"
    protocol: Literal["gcs"] = "gcs"
    args: list = []
    path: Path = Path("/")
    project: str = GCSFS_DEFAULT_PROJECT  # GCS project_id (not project name)
    access: Scopes = Scopes.READ_ONLY
    token: str | dict | None = None
    endpoint_url: str | None = None
    timeout: float | None = None

    def init(self) -> FSBackend:
        return GCSBackend(self.filesystem(), path_base=self.path)

    def filesystem(self) -> GCSFileSystem:
        return GCSFileSystem.from_json(self.model_dump_json())
